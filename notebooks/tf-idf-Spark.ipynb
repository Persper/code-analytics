{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analyses of Spark PRs with spaCy and Scikit-Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to analyze the Spark commit messages and PRs with spaCy and Scikit-Learn. \n",
    "\n",
    "First, we extract the useful descriptions and comments from Spark issues (Section 1). \n",
    "\n",
    "Second, we use spaCy to do tokenization, remove stop words and reduce inflected words to their root stem (Section 2). \n",
    "\n",
    "Third, we use Scikit-Learn to analyze Spark issues: calculate the percentages of different kinds of words in all issues (Section 3.1), the TF-IDF of all issues (Section 3.2), the term frequency distributions (Section 3.3), the classification of all issues (Section 3.4) and the issue similarity matrix (Section 3.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Linux-like system, (including, to greater or lesser degrees, Ubuntu, MacOS, Cygwin, and Bash for Windows), you should be able to run these commands to install SpaCy, Scikit-Learn, Pandas, and the other required libraries. Ete3 is a library for tree visualization which is optional.\n",
    "\n",
    "    sudo pip install spacy scikit-learn pandas ete3\n",
    "\n",
    "Now download the SpaCy data with this command:\n",
    "\n",
    "    python -m spacy.en.download all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract the useful information from Spark issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import xml.dom.minidom\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some useless XML headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(data):\n",
    "    data = re.sub(r'</?p>', \"\", data)\n",
    "    data = re.sub(r'</?tt>', \"\", data)\n",
    "    data = re.sub(r'<br/>', \"\", data)\n",
    "    data = re.sub(r'\\<a.*?\\>', \"\", data)\n",
    "    data = re.sub(r'</a>', \"\", data)\n",
    "    data = re.sub(r'\\<div.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\</div\\>', \"\", data)\n",
    "    data = re.sub(r'\\<pre.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\</pre\\>', \"\", data)\n",
    "    data = re.sub(r'\\<span.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\</span\\>', \"\", data)\n",
    "    data = re.sub(r'\\<ul.*?\\>', \"\", data)\n",
    "    data = re.sub(r'</ul\\>', \"\", data)\n",
    "    data = re.sub(r'\\<table.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\</table\\>', \"\", data)\n",
    "    data = re.sub(r'\\<td.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\</td\\>', \"\", data)\n",
    "    data = re.sub(r'\\<th.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\</th\\>', \"\", data)\n",
    "    data = re.sub(r'\\</?del\\>', \"\", data)\n",
    "    data = re.sub(r'\\</?em\\>', \"\", data)\n",
    "    data = re.sub(r'\\</?h3\\>', \"\", data)\n",
    "    data = re.sub(r'\\</?li\\>', \"\", data)\n",
    "    data = re.sub(r'</?ol>', \"\", data)\n",
    "    data = re.sub(r'</?tr>', \"\", data)\n",
    "    data = re.sub(r'</?tbody>', \"\", data)\n",
    "    data = re.sub(r'\\<img.*?\\>', \"\", data)\n",
    "    data = re.sub(r'\\n', \" \", data)\n",
    "    data = re.sub(r'\\&gt\\;', \">\", data)\n",
    "    data = re.sub(r'\\&lt\\;', \"<\", data)\n",
    "    data = re.sub(r'\\&\\#91\\;', \"[\", data)\n",
    "    data = re.sub(r'\\&\\#93\\;', \"]\", data)\n",
    "    data = re.sub(r'\\&\\#8211\\;', \"-\", data)\n",
    "    data = re.sub(r'\\&amp\\;', \"&\", data)\n",
    "    data = re.sub(r'\\<200c\\>', \"\", data)\n",
    "    data = re.sub(r'\\<200b\\>', \"\", data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read titles, descriptions and comments from Spark issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInfoFromXML(root, toFile):\n",
    "    fopen = open(toFile, 'w')\n",
    "    title = root.getElementsByTagName('title')\n",
    "    for i, ti in enumerate(title):\n",
    "        if i != 0:\n",
    "            data = ti.firstChild.data\n",
    "            data = re.sub(r'\\[.*?\\]\\s', \"\", data)\n",
    "            data = remove(data)\n",
    "            fopen.write('%s\\n' % (data))\n",
    "    description = root.getElementsByTagName('description')\n",
    "    for i, des in enumerate(description):\n",
    "        if i != 0 and des.firstChild != None:\n",
    "            data = remove(des.firstChild.data)\n",
    "            fopen.write('%s\\n' % (data.encode('utf-8')))\n",
    "    comments = root.getElementsByTagName('comment')\n",
    "    for i, com in enumerate(comments):\n",
    "        data = remove(com.firstChild.data)\n",
    "        fopen.write('%s\\n' % (data.encode('utf-8')))\n",
    "    fopen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the extracted information to new dir and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoFromXML(fromDir, path):\n",
    "    toDir = \"../data/desAndCom/\"\n",
    "    fromFile = os.path.join('%s%s' % (fromDir, path))\n",
    "    toFile = os.path.join('%s%s' % (toDir, path))\n",
    "    #print fromFile, toFile, path\n",
    "    dom = xml.dom.minidom.parse(fromFile)\n",
    "    root = dom.documentElement\n",
    "    readInfoFromXML(root, toFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all files of this dir 'filepath'\n",
    "def eachFile(filepath):\n",
    "    pathDir =  os.listdir(filepath)\n",
    "    for allDir in pathDir:\n",
    "        child = os.path.join('%s%s' % (filepath, allDir))\n",
    "        print child.decode('gbk')\n",
    "\n",
    "# Print the content of this file 'filename'\n",
    "def readFile(filename):\n",
    "    fopen = open(filename, 'r') \n",
    "    for eachLine in fopen:\n",
    "        print \"the content of this line\", line\n",
    "    fopen.close()\n",
    "\n",
    "# Write multiple lines to a specific file\n",
    "def writeFile(filename):\n",
    "    fopen = open(filename, 'w')\n",
    "    while True:\n",
    "        aLine = raw_input()\n",
    "        if aLine != \".\":\n",
    "            fopen.write('%s%s' % (aLine, os.linesep))\n",
    "        else:\n",
    "            print \"the file is saved\"\n",
    "            break\n",
    "    fopen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all descriptions and comments for spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsefulInfo(filepath):\n",
    "    pathDir =  os.listdir(filepath)\n",
    "    subName = \"SPARK\"              # now only deal with the spark issue \n",
    "    invalidName = \"invalid\"\n",
    "    numFiles = 0\n",
    "    for allDir in pathDir:\n",
    "        if subName in allDir:\n",
    "            if invalidName in allDir:\n",
    "                continue\n",
    "            #print allDir\n",
    "            getInfoFromXML(filepath, allDir)\n",
    "            numFiles += 1\n",
    "    return numFiles\n",
    "            #print allDir\n",
    "\n",
    "filePath = \"../data/spark-issues/\"\n",
    "#eachFile(filePath)\n",
    "numFiles = getUsefulInfo(filePath)             #write all useful information to ../data/desAndCom\n",
    "numFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use spaCy to do tokenization, stemming and remove stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will load the model of spaCy, which might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " nlp = spacy.load('en-core-web-md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all extracted files and print the number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList =  os.listdir(\"../data/desAndCom/\")\n",
    "fileDir = \"../data/desAndCom/\"\n",
    "files = []\n",
    "for tempFile in fileList:\n",
    "    if \"ipynb\" in tempFile:\n",
    "        continue\n",
    "    files.append(os.path.join('%s%s' % (fileDir, tempFile)))\n",
    "print len(files)  \n",
    "#print files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the texts. These commands might take a little while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spaCy to analysis these files\n",
    "text_num = len(files)\n",
    "text_array = [[]] * text_num\n",
    "for i, tempFile in enumerate(files):\n",
    "    raw_data = open(files[i]).read()\n",
    "    text_array[i] = nlp(raw_data.decode('utf-8'))\n",
    "#print files[0]\n",
    "#print text_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for checking\n",
    "#for token in text_array[0]:\n",
    "#    print (token, token.lemma_, token.lemma, token.pos_, token.pos, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each SpaCy document is already tokenized into words, which are accessible by iterating over the document. The next step just prints one text after removing stop words, punctuations, bracket, etc. Note capitals and steming will be dealt later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(text_array[0]):\n",
    "    if token.is_punct or token.is_space or token.is_stop or token.pos_ == 'SYM':\n",
    "        continue\n",
    "    print (token, token.lemma_, token.lemma, token.pos_, token.pos, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the intermediate results to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = os.listdir(\"../data/desAndCom/\")\n",
    "fileDir = \"../data/clearFiles/\"\n",
    "files = []\n",
    "for tempFile in fileList:\n",
    "    if \"ipynb\" in tempFile:\n",
    "        continue\n",
    "    files.append(os.path.join('%s%s' % (fileDir, tempFile)))\n",
    "\n",
    "for j in range(text_num):\n",
    "    fopen = open(files[j], 'w')\n",
    "    for i, token in enumerate(text_array[j]):\n",
    "        if token.is_punct or token.is_space or token.is_stop or token.pos_ == 'SYM':   #include symbor, such as \"=\"\n",
    "            continue\n",
    "        fopen.write(token.lemma_.encode('utf8') + \" \")   #reduce all words to its root stem\n",
    "        #print token.lemma_\n",
    "    fopen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Scikit-Learn to analyze PRs and issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the clear files and use the spaCy models to analyze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_num = len(files)\n",
    "text_array = [] * text_num\n",
    "for i, tempFile in enumerate(files):\n",
    "    raw_data = open(files[i]).read()\n",
    "    text_array.append(nlp(raw_data.decode('utf-8')))\n",
    "#print files[0]\n",
    "#print text_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.  The percentage of different POS (Parts of Speech) in all issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word already has a part of speech and a tag associated with it. It's fun to compare the distribution of parts of speech in all issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of all words\n",
    "totalWords = 0\n",
    "for i, tempFile in enumerate(files):\n",
    "    totalWords += len(text_array[i])\n",
    "#print totalWords\n",
    "\n",
    "# Get the total number of different POS\n",
    "totalType = {}\n",
    "for i, tempFile in enumerate(files):\n",
    "    typeMap = text_array[i].count_by(spacy.attrs.POS)\n",
    "    for obj in typeMap.items():\n",
    "        if obj[0] in totalType:\n",
    "            totalType[obj[0]] += obj[1]\n",
    "        else:\n",
    "            totalType[obj[0]] = obj[1]\n",
    "\n",
    "# Test\n",
    "#for obj in totalType.items():       \n",
    "#    print obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal axis shows the parts of speech, and the vertical axis shows the percentage of different kinds of words. The noun, the adjective, the number and the verb account for the most proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data of the figure\n",
    "textPOS = [] \n",
    "textPOS.append(pd.Series(totalType) / totalWords)     # the sequence of POS percentages\n",
    "#print textPOS\n",
    "\n",
    "# Set the tag in the X axis\n",
    "tagDict = {}\n",
    "for i, tempFile in enumerate(files):\n",
    "    for w in text_array[i]:\n",
    "        tagDict[w.pos] = w.pos_\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "df = pd.DataFrame([textPOS[0]], index=['Spark'])                        # the figure configuration\n",
    "df.columns = [tagDict[column] for column in df.columns]                 # the columns configuration\n",
    "df.T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the percentage of different POS in two issues to draw a picture\n",
    "#textPOS = [] * text_num\n",
    "#for i, tempFile in enumerate(files):\n",
    "#    textPOS.append(pd.Series(text_array[i].count_by(spacy.attrs.POS))/len(text_array[i]))     # the sequence of POS percentages\n",
    "#rcParams['figure.figsize'] = 16, 8\n",
    "#df = pd.DataFrame([textPOS[0], textPOS[1]], index=['firstText', 'secondText'])  # the figure configuration\n",
    "#df.columns = [tagDict[column] for column in df.columns]                 # the columns configuration\n",
    "#df.T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.  The TF-IDF of all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses a non-semantic technique for vectorizing documents, just using bag-of-words. We won't need any of the fancy features of SpaCy for this, just scikit-learn. We'll vectorize the corpus using scikit-learn's TfidfVectorizer class. This creates a matrix of word frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll vectorize the corpus using scikit-learn's TfidfVectorizer class.\n",
    "tfidf = TfidfVectorizer(input='filename', decode_error='ignore', use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFilenames = sorted(glob('../data/clearFiles/*'))\n",
    "#print testFilenames\n",
    "\n",
    "# While we're at it, let's make a list of the lengths, so we can use them to plot dot sizes. \n",
    "lengths = [len(open(filename).read())/100 for filename in testFilenames]\n",
    "#print lengths\n",
    "\n",
    "# Add a manually compiled list of presidential party affiliations, \n",
    "# So that we can use this to color our dots. \n",
    "parties = 'rrrbbrrrbbbbbrrbbrrbrrrbbrrbrrrrbbrrrbbbbbrrbbrrbrrrbbrrbrrrrbbrrrbbbbbrrbbrrbrrrbbrrbrrrrbbrrrbbbbbrrbbrrbrrrbbrrbr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfOut = tfidf.fit_transform(testFilenames)\n",
    "tfidfOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the table shows, because the number of words is huge, we only print the top 5 TF-IDF words of all issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "#print feature_names\n",
    "\n",
    "#print the TFIDF of two articles\n",
    "#print '\\n------------------------ the TFIDF of all words of two issues ---------------------------------'\n",
    "temp = 0\n",
    "feature_index = tfidfOut[temp,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [tfidfOut[temp, x] for x in feature_index])\n",
    "#for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "#    print w, s\n",
    "#print '-----------------------------------------------------------------------'\n",
    "temp = 1\n",
    "feature_index = tfidfOut[temp,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [tfidfOut[temp, x] for x in feature_index])\n",
    "#for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "#    print w, s\n",
    "\n",
    "print '\\n------------------------ the top 5 TFIFP words of all issues ---------------------------------'\n",
    "for i, tfid in enumerate(tfidfOut):\n",
    "    feature_index = tfidfOut[i,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidfOut[i, x] for x in feature_index])\n",
    "    sorted_l=sorted(tfidf_scores,key=lambda t:t[1], reverse=True)  \n",
    "    print \"the %dth Spark issue:\" % i\n",
    "    numOut = 0\n",
    "    for w, s in [(feature_names[j], s) for (j, s) in sorted_l]:\n",
    "        print (w, s), \n",
    "        numOut += 1\n",
    "        if numOut > 5:\n",
    "            print \"\\n\"\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of TFIDF of all documents. For example, 93 is the number of issues and 3005 is the number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfOut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.  The term frequency distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're simply going to count the occurrences of words and divide by the total number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make labels by removing the directory name and .txt/.xml extension: \n",
    "labels = [filename.split('/')[3] for filename in testFilenames]\n",
    "labels = [filename.split('.')[0] for filename in labels]\n",
    "#print labels\n",
    "\n",
    "# We're simply going to count the occurrences of words and divide by the total number of words in the document.\n",
    "doc_raw = [open(doc).read() for doc in testFilenames]\n",
    "inaugural = [nlp(doc.decode(\"utf-8\")) for doc in doc_raw]\n",
    "\n",
    "# Create a Pandas Data Frame with each word counted in each document, divided by the length of the document. \n",
    "inauguralSeries = [pd.Series(Counter([word.string.strip().lower() for word in doc])) / len(doc) for doc in inaugural]\n",
    "seriesDict = {label: series for label, series in zip(labels, inauguralSeries)}\n",
    "inauguralDf = pd.DataFrame(seriesDict).T.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know the frequencency of each word in first 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inauguralDf.head()\n",
    "# you can know the frequencency of each word in all documents\n",
    "# inauguralDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily slice this data frame with words we're interested in, and plot those words across the corpus. For example, let's look at the proportions of the words \"important\", \"key\" and \"lose\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inauguralDf[['important', 'key', 'lose']].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even compute, say the ratio of uses of the word \"master\" to uses of the word \"class.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#americaWorldRatio = inauguralDf['master']/inauguralDf['class']\n",
    "#americaWorldRatio.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.  The issues classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfOut[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuase a word vector is 3005-dimensional, so in order to plot it in 2D, it might help to reduce the dimensionality to the most meaningful dimensions. We can use Scikit-Learn to perform truncated singular value decomposition for latent semantic analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=2)\n",
    "lsaOut = lsa.fit_transform(tfidfOut.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classification of all documents\n",
    "xs, ys = lsaOut[:,0], lsaOut[:,1]\n",
    "for i in range(len(xs)): \n",
    "    plt.scatter(xs[i], ys[i], c=parties[0], s=lengths[i], alpha=0.5)\n",
    "    plt.annotate(labels[i], (xs[i], ys[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.  The document similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the .similarity() method from earlier that uses word vectors, we can very easily compute the document similarity between all the documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　Document Similarity Matrix\n",
    "similarities = [ [doc.similarity(other) for other in inaugural] for doc in inaugural ]\n",
    "similaritiesDf = pd.DataFrame(similarities, columns=labels, index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the horizontal axis and the vertical axis show all issues, the deeper the color, the more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires the Seaborn library. \n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "seaborn.heatmap(similaritiesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top PROPN words\n",
    "#firstAdjs = [w for w in first if w.pos_ == 'PROPN']\n",
    "#Counter([w.string.strip() for w in firstAdjs]).most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
