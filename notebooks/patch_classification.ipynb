{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pickle\n",
    "datasets = pickle.load(open(\"../data/fs-patch/fs_datasets.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.svm import SVC\n",
    "import functools\n",
    "\n",
    "def binary_bug(dp):\n",
    "    if dp['type'] == 'b':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def multi_patch_type(dp):\n",
    "    return dp['type']\n",
    "\n",
    "def multi_bug_cons(dp):\n",
    "    return dp['cons_type']\n",
    "\n",
    "def only_bug_filter(dp):\n",
    "    return dp['type'] == 'b'\n",
    "        \n",
    "rf = functools.partial(sklearn.ensemble.RandomForestClassifier, n_estimators=300)\n",
    "svm = functools.partial(SVC, kernel='linear')\n",
    "\n",
    "def leave_one_dataset_out(datasets, text_feature, label_func, init_clf, \n",
    "                          ngram_range=(1, 1), extra_features=None, \n",
    "                          dp_filter=lambda dp: True):\n",
    "    \"\"\"Prepare datasets for training in a leave-one-out style\n",
    "    \n",
    "    Args:\n",
    "        datasets: A dictionary, keys are dataset names, each dataset\n",
    "            is a list of data points (also dictionaries).\n",
    "        text_feature: A string, can be either 'message' or 'subject'.\n",
    "        label_func: A function, takes a data point as input and\n",
    "            return its target label.\n",
    "        init_clf: A function, return a classifier which supports\n",
    "            'fit' and 'score' method\n",
    "        extra_features: A list of strings.\n",
    "        dp_filter: A function decides which data point to exclude.\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    assert(text_feature in ('message', 'subject'))\n",
    "    \n",
    "    train_texts = {}\n",
    "    train_extras = {}\n",
    "    train_targets = {}\n",
    "    train_vectors = {}\n",
    "    \n",
    "    test_texts = {}\n",
    "    test_extras = {}\n",
    "    test_targets = {}\n",
    "    test_vectors = {}\n",
    "    \n",
    "    classifiers = {}\n",
    "    vectorizers = {}\n",
    "\n",
    "    for fs in fss:\n",
    "        train_texts[fs] = []\n",
    "        train_targets[fs] = []\n",
    "        train_extras[fs] = []\n",
    "        for fs2 in fss:\n",
    "            if fs2 != fs:\n",
    "                for dp in datasets[fs2]:\n",
    "                    if text_feature in dp and dp_filter(dp):\n",
    "                        train_texts[fs].append(dp[text_feature])\n",
    "                        train_targets[fs].append(label_func(dp))\n",
    "                        \n",
    "\n",
    "        test_texts[fs] = []\n",
    "        test_targets[fs] = []\n",
    "        for dp in datasets[fs]:\n",
    "            if text_feature in dp and dp_filter(dp):\n",
    "                test_texts[fs].append(dp[text_feature])\n",
    "                test_targets[fs].append(label_func(dp))\n",
    "            \n",
    "        vectorizers[fs] = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=ngram_range)\n",
    "        train_vectors[fs] = vectorizers[fs].fit_transform(train_texts[fs])\n",
    "        test_vectors[fs] = vectorizers[fs].transform(test_texts[fs])\n",
    "\n",
    "        classifiers[fs] = init_clf()\n",
    "        classifiers[fs].fit(train_vectors[fs], train_targets[fs])\n",
    "\n",
    "        print('----- Test Accuracy for %s -----' % fs)\n",
    "        print('Classifier: %.3f' % classifiers[fs].score(test_vectors[fs], test_targets[fs]))\n",
    "\n",
    "        pred2 = []\n",
    "        for text in test_texts[fs]:\n",
    "            if 'fix' in text.lower() or 'fixes' in text.lower() or 'fixed' in text.lower():\n",
    "                pred2.append(1)\n",
    "            else:\n",
    "                pred2.append(0)\n",
    "        print('Naive: %.3f' % sklearn.metrics.accuracy_score(test_targets[fs], pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leave_one_dataset_out(datasets, 'subject', binary_bug, svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leave_one_dataset_out(datasets, 'message', binary_bug, svm, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leave_one_dataset_out(datasets, 'subject', multi_patch_type, svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leave_one_dataset_out(datasets, 'subject', multi_bug_cons, svm, dp_filter=only_bug_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining with lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizers['ext3'], classifiers['ext3'])\n",
    "print(c.predict_proba([test_texts['ext3'][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names = ['not-bug', 'bug']\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "def explain_pred(idx, fs):\n",
    "    c = make_pipeline(vectorizers[fs], classifiers[fs])\n",
    "    exp = explainer.explain_instance(test_texts[fs][idx].lower(), c.predict_proba, num_features=8)\n",
    "    print('Patch id: %d' % idx)\n",
    "    print('Probability(bug) =', c.predict_proba([test_texts[fs][idx]])[0,1])\n",
    "    print('True class: %s' % class_names[test_targets[fs][idx]])\n",
    "    print('Text: %s' % test_texts[fs][idx])\n",
    "    pp.pprint(exp.as_list())\n",
    "    # exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = vectorizers['ext3'].vocabulary_\n",
    "names = [None] * len(vocab)\n",
    "for v in vocab:\n",
    "    names[vocab[v]] = v\n",
    "pp.pprint(sorted(zip(map(lambda x: '%.4f' % x, classifiers['ext3'].feature_importances_), names), reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "fs = 'ext3'\n",
    "c = make_pipeline(vectorizers[fs], classifiers[fs])\n",
    "for i in range(len(test_texts[fs])):\n",
    "    if ('fix' not in test_texts[fs][i].lower() \n",
    "        and c.predict_proba([test_texts[fs][i]])[0,1] > 0.5\n",
    "        and test_targets[fs][i] == 1):\n",
    "        explain_pred(i, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keyword 'fix'\n",
    "for i in [22, 24]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when keyword 'fix' is absent\n",
    "for i in [23, 25]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# interesting case\n",
    "for i in [5, 26]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for commit in list(r.iter_commits(rev='v2.6.12')):\n",
    "    if 'Linux 2.6.0' in commit.message.split('\\n', 1)[0]:\n",
    "        print(commit.hexsha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b'\\xd0\\x9f\\xe4\\nG\\xb5\\xe8G6\\x88(\\x95e\\xac\\xd4\\xb2Q\\r\\xd8b'\n",
    "[PATCH] ext3: fix determination of inode journalling mode"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
