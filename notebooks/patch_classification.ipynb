{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pickle\n",
    "datasets = pickle.load(open(\"../data/fs-patch/fs_datasets.pickle\", 'rb'))\n",
    "fss = ['ext3', 'ext4', 'btrfs', 'xfs', 'jfs', 'reiserfs']\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize, binarize, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "import functools\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "rf = functools.partial(sklearn.ensemble.RandomForestClassifier, n_estimators=300)\n",
    "\n",
    "def binary_bug(dp):\n",
    "    if dp['type'] == 'b':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def multi_patch_type(dp):\n",
    "    return dp['type']\n",
    "\n",
    "def multi_bug_cons(dp):\n",
    "    return dp['cons_type']\n",
    "\n",
    "def only_bug_filter(dp):\n",
    "    return dp['type'] == 'b'\n",
    "        \n",
    "def lemmatize(text):\n",
    "    lemmatized = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PUNCT':\n",
    "            lemmatized.append(token.lemma_)\n",
    "    return lemmatized\n",
    "\n",
    "def stem(text):\n",
    "    stemmed = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed\n",
    "\n",
    "def bns(tprs, fprs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tprs: A row vector of shape (1, num_features)\n",
    "        fprs: A row vector of shape (1, num_features)\n",
    "    \"\"\"\n",
    "    num_features = tprs.shape[1]\n",
    "    bns = np.zeros(num_features)\n",
    "    for i in range(num_features):\n",
    "        bns[i] = np.abs(norm.ppf(tprs[0, i]) - norm.ppf(fprs[0, i]))\n",
    "    return bns\n",
    "\n",
    "def tfbns(tfs, bns):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tfs: A sparse matrix of shape (num_samples, num_features)\n",
    "            in csr format\n",
    "        bns: A numpy array of shape (num_features,)\n",
    "    \"\"\"\n",
    "    cx = tfs.tocoo()\n",
    "    data, rows, cols = [], [], []\n",
    "    for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "        data.append(v * bns[j])\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "    return csr_matrix((data, (rows, cols)), shape=tfs.shape)\n",
    "\n",
    "class Experiment():\n",
    "    \n",
    "    def __init__(self, datasets, fss):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datasets: A dictionary, keys are dataset names, each dataset\n",
    "                is a list of data points (also dictionaries).\n",
    "            fss: A list of file system names.\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.fss = fss\n",
    "\n",
    "    def _clean(self):\n",
    "        self.train_texts = {}\n",
    "        self.train_extras = {}\n",
    "        self.train_targets = {}\n",
    "        self.train_tfidfs = {}\n",
    "        self.train_vectors = {}\n",
    "        self.train_counts = {}\n",
    "        self.train_tfs = {}\n",
    "        self.train_tfbns = {}\n",
    "        \n",
    "        self.test_texts = {}\n",
    "        self.test_extras = {}\n",
    "        self.test_targets = {}\n",
    "        self.test_tfidfs = {}\n",
    "        self.test_vectors = {}\n",
    "        self.test_counts = {}\n",
    "        self.test_tfs = {}\n",
    "        self.test_tfbns = {}\n",
    "\n",
    "        self.vectorizers = {}\n",
    "        self.tf_transformers = {}\n",
    "        self.feature_labels = {}\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def leave_one_dataset_out(self, label_func, init_clf, ngram_range=(1, 1), \n",
    "                              text_feature='message', extra_features=None, \n",
    "                              dp_filter=lambda dp: True, tokenizer=None, \n",
    "                              max_features=None, min_df=1):\n",
    "        \"\"\"Perform experiment in a leave-one-out style\n",
    "        \n",
    "        Args:\n",
    "            label_func: A function, takes a data point as input and\n",
    "                return its target label.\n",
    "            init_clf: A function, return a classifier which supports\n",
    "                'fit' and 'score' method\n",
    "            text_feature: A string, can be either 'message' or 'subject'.\n",
    "                If set to None, then texts will not be used.\n",
    "            extra_features: A list of strings.\n",
    "            dp_filter: A function decides which data point to exclude.\n",
    "            tokenizer: A function takes a string and return a list of tokens.\n",
    "            max_features: An int or None. If not None, only consider\n",
    "                top max_features ordered by term frequency across the corpus.\n",
    "            min_df: An int, ignore terms when building vocabulary if their\n",
    "                document frequency is strictly lower than this threshold.\n",
    "        \"\"\"\n",
    "        \n",
    "        def gather_data(fs, text_list, target_list, extra_list):\n",
    "            \"\"\"Helper function to gather data from a single dataset\"\"\"\n",
    "            for dp in self.datasets[fs]:\n",
    "                if dp_filter(dp):\n",
    "                    if extra_features and 'message' in dp:\n",
    "                        if text_feature:\n",
    "                            text_list.append(dp[text_feature])\n",
    "                        target_list.append(label_func(dp))\n",
    "                        extra_list.append([dp[f] for f in extra_features])\n",
    "                    elif not extra_features and text_feature in dp:\n",
    "                        text_list.append(dp[text_feature])\n",
    "                        target_list.append(label_func(dp))\n",
    "                \n",
    "        assert((text_feature and text_feature in ('message', 'subject')) or extra_features)\n",
    "        self._clean()\n",
    "\n",
    "        for fs in self.fss:\n",
    "\n",
    "            self.train_texts[fs], self.train_targets[fs], \\\n",
    "                self.train_extras[fs] = [], [], []\n",
    "            self.test_texts[fs], self.test_targets[fs], \\\n",
    "                self.test_extras[fs] = [], [], []\n",
    "\n",
    "            for fs2 in self.fss:\n",
    "                if fs2 != fs:\n",
    "                    gather_data(fs2, self.train_texts[fs],\n",
    "                        self.train_targets[fs], self.train_extras[fs])\n",
    "\n",
    "            gather_data(fs, self.test_texts[fs], self.test_targets[fs],\n",
    "                self.test_extras[fs])\n",
    "\n",
    "            self.feature_labels[fs] = []\n",
    "\n",
    "            if text_feature:\n",
    "                # vectorize texts\n",
    "                self.vectorizers[fs] = CountVectorizer(\n",
    "                                            tokenizer=tokenizer,\n",
    "                                            ngram_range=ngram_range,\n",
    "                                            max_features=max_features,\n",
    "                                            min_df=min_df)\n",
    "                \n",
    "                self.train_counts[fs] = \\\n",
    "                    self.vectorizers[fs].fit_transform(self.train_texts[fs])\n",
    "                self.test_counts[fs] = \\\n",
    "                    self.vectorizers[fs].transform(self.test_texts[fs])\n",
    "    \n",
    "                self.tf_transformers[fs] = TfidfTransformer(use_idf=False, norm=None)\n",
    "                \n",
    "                self.train_tfs[fs] = \\\n",
    "                    self.tf_transformers[fs].fit_transform(self.train_counts[fs])\n",
    "                self.test_tfs[fs] = \\\n",
    "                    self.tf_transformers[fs].transform(self.test_counts[fs])\n",
    "                    \n",
    "                binary_train_counts = binarize(self.train_counts[fs])\n",
    "                    \n",
    "                classes = np.unique(self.train_targets[fs])\n",
    "                # label_binarize return column vector of shape (num_samples, 1)\n",
    "                binary_train_targets = label_binarize(self.train_targets[fs], classes)\n",
    "                \n",
    "                train_pos = np.sum(binary_train_targets)\n",
    "                train_neg = binary_train_targets.shape[0] - train_pos\n",
    "                \n",
    "                # train_tp is a row vector of shape (1, num_features)\n",
    "                train_tps = np.sum(binary_train_counts[np.nonzero(binary_train_targets)[0]], axis=0)\n",
    "                train_fps = np.sum(binary_train_counts[np.argwhere(binary_train_targets == 0)[:, 0]], axis=0)\n",
    "                \n",
    "                train_tprs = np.clip(train_tps / train_pos, 0.0005, 0.9995)\n",
    "                train_fprs = np.clip(train_fps / train_neg, 0.0005, 0.9995)\n",
    "                train_bns = bns(train_tprs, train_fprs)\n",
    "                \n",
    "                self.train_tfbns[fs] = tfbns(self.train_tfs[fs], train_bns)\n",
    "                self.test_tfbns[fs] = tfbns(self.test_tfs[fs], train_bns)\n",
    "                \n",
    "                normalizer = Normalizer()\n",
    "                self.train_tfbns[fs] = normalizer.transform(self.train_tfbns[fs])\n",
    "                self.test_tfbns[fs] = normalizer.transform(self.test_tfbns[fs])\n",
    "            \n",
    "                \"\"\"\n",
    "                self.vectorizers[fs] = TfidfVectorizer(\n",
    "                                            tokenizer=tokenizer, \n",
    "                                            ngram_range=ngram_range, \n",
    "                                            max_features=max_features)\n",
    "                self.train_tfidfs[fs] = \\\n",
    "                    self.vectorizers[fs].fit_transform(self.train_texts[fs])\n",
    "                self.test_tfidfs[fs] = \\\n",
    "                    self.vectorizers[fs].transform(self.test_texts[fs])\n",
    "                \"\"\"\n",
    "\n",
    "                vocab = self.vectorizers[fs].vocabulary_\n",
    "                self.feature_labels[fs] = [None] * len(vocab)\n",
    "                for v in vocab:\n",
    "                    self.feature_labels[fs][vocab[v]] = v\n",
    "\n",
    "            if extra_features:\n",
    "                self.train_extras[fs] = np.array(self.train_extras[fs])\n",
    "                self.test_extras[fs] = np.array(self.test_extras[fs])\n",
    "\n",
    "                # scale to have zero mean and unit variance\n",
    "                scaler = StandardScaler()\n",
    "                self.train_extras[fs] = scaler.fit_transform(self.train_extras[fs])\n",
    "                self.test_extras[fs] = scaler.transform(self.test_extras[fs])\n",
    "\n",
    "                for f in extra_features:\n",
    "                    self.feature_labels[fs].append(f)\n",
    "                    \n",
    "            if text_feature and extra_features:\n",
    "                self.train_vectors[fs] = \\\n",
    "                    np.hstack((self.train_tfbns[fs].todense(), self.train_extras[fs]))\n",
    "                self.test_vectors[fs] = \\\n",
    "                    np.hstack((self.test_tfbns[fs].todense(), self.test_extras[fs]))\n",
    "            elif text_feature and not extra_features:\n",
    "                self.train_vectors[fs] = self.train_tfbns[fs]\n",
    "                self.test_vectors[fs] = self.test_tfbns[fs]\n",
    "            elif not text_feature and extra_features:\n",
    "                self.train_vectors[fs] = self.train_extras[fs]\n",
    "                self.test_vectors[fs] = self.test_extras[fs]\n",
    "        \n",
    "\n",
    "            self.classifiers[fs] = init_clf()\n",
    "            self.classifiers[fs].fit(self.train_vectors[fs], self.train_targets[fs])\n",
    "\n",
    "            print('----- Test Accuracy for %s -----' % fs)\n",
    "            print('Classifier: %.3f' % self.classifiers[fs].score(\n",
    "                self.test_vectors[fs], self.test_targets[fs]))\n",
    "\n",
    "            \"\"\"\n",
    "            pred2 = []\n",
    "            for text in self.test_texts[fs]:\n",
    "                if 'fix' in text.lower(): \n",
    "                    pred2.append(1)\n",
    "                else:\n",
    "                    pred2.append(0)\n",
    "            print('Naive: %.3f' % sklearn.metrics.accuracy_score(\n",
    "                self.test_targets[fs], pred2))\n",
    "            \"\"\"\n",
    "        \n",
    "        print('Average number of features: %.1f' % \n",
    "              np.mean(np.array([len(self.feature_labels[fs]) for fs in fss])))\n",
    "            \n",
    "exp = Experiment(datasets, fss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification of Bug Fix Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, only using features derived from code\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, text_feature=None,\n",
    "                          extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, only using text feature\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, only using text feature \n",
    "# and drop terms with frequency lower than min_df\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, using text feature with features derived from code\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, \n",
    "                          extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, using stemmed text feature and setting max_features to 5000\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, tokenizer=stem, max_features=5000,\n",
    "                         extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, using bigram text feature with features derived from code\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, ngram_range=(1, 2), \n",
    "                          extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classification of bug patch, using lemmatized text feature with features derived from code\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, tokenizer=lemmatize,\n",
    "                          extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binary classfication of bug patch, using stemmed text feature with features derived from code\n",
    "exp.leave_one_dataset_out(binary_bug, LinearSVC, tokenizer=stem, \n",
    "                          extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification of Patch Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multi-class classification of patch type, only using text feature\n",
    "exp.leave_one_dataset_out(multi_patch_type, LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multi-class classification of patch type, using text feature with features derived from code\n",
    "exp.leave_one_dataset_out(multi_patch_type, LinearSVC,\n",
    "                         extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification of Bug Consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multi-class classification of bug consequences, only using text feature\n",
    "exp.leave_one_dataset_out(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multi-class classification of bug consequences, using bigram text feature\n",
    "exp.leave_one_dataset_out(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multi-class classification of bug consequences, using text feature with features derived from code\n",
    "exp.leave_one_dataset_out(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter,\n",
    "                         extra_features=['num_files', 'num_adds', 'num_dels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explaning random forest classifier\n",
    "def rank_features_by_importance(exp, top_n=20):\n",
    "    if 'feature_importances_' in dir(exp.classifiers['ext3']):\n",
    "        for fs in fss:\n",
    "            print('------- important features for %s -------' % fs)\n",
    "            truncated_importances = map(lambda x: '%.4f' % x, exp.classifiers[fs].feature_importances_)\n",
    "            pp.pprint(sorted(zip(truncated_importances, exp.feature_labels[fs]), reverse=True)[:top_n])\n",
    "    else:\n",
    "        print(\"classifiers don't have attribute feature_importance_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_features_by_importance(exp, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explaining linear SVM classifier\n",
    "def rank_features_by_weight(classifiers, classes, feature_labels, \n",
    "                            top_n=20, individual_class=False):\n",
    "    \"\"\"Rank features by the absolute value of associated primal weight\n",
    "    \n",
    "    Args:\n",
    "        classifiers: A dictionary, with key being the file system name\n",
    "            and value being the corresponding classifier.\n",
    "        classes: A list of class labels.\n",
    "        feature_labels: A dictionary of list of feature names, each key\n",
    "            is a file system name, features are in the order as classifier\n",
    "            sees them.\n",
    "        top_n: An integer, specifies number of top features to print.\n",
    "        individual_class: A boolean value, whether use sum of absolute weights \n",
    "            across classes. In binary tasks, this value doesn't matter.\n",
    "    \"\"\"\n",
    "    \n",
    "    fss = list(classifiers.keys())\n",
    "    if 'coef_' in dir(classifiers[fss[0]]):\n",
    "        for fs in fss:\n",
    "            print('------- %s -------' % fs)\n",
    "            if individual_class:\n",
    "                coef = classifiers[fs].coef_\n",
    "                for i in range(coef.shape[0]):\n",
    "                    print('\\t------ %s ------' % classes[i])\n",
    "                    # print('\\t------  ------')\n",
    "                    order = np.argsort(np.absolute(coef[i]))\n",
    "                    pp.pprint(np.array(feature_labels[fs])[order][-top_n:])\n",
    "                print()\n",
    "            else:\n",
    "                coef = classifiers[fs].coef_\n",
    "                num_features = coef.shape[1]\n",
    "                abs_coef_sum = np.zeros(num_features)\n",
    "                for i in range(coef.shape[0]):\n",
    "                    abs_coef_sum += np.absolute(coef[i])\n",
    "                order = np.argsort(abs_coef_sum)\n",
    "                pp.pprint(np.array(feature_labels[fs])[order][-top_n:])\n",
    "    else:\n",
    "        print(\"classifiers don't have attribute coef_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rank_features_by_weight(exp.classifiers, np.unique(exp.train_targets['ext3']), \n",
    "                        exp.feature_labels, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rank_features_by_weight(exp.classifiers, np.unique(exp.train_targets['ext3']), \n",
    "                        exp.feature_labels, top_n=20, individual_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizers['ext3'], classifiers['ext3'])\n",
    "print(c.predict_proba([test_texts['ext3'][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names = ['not-bug', 'bug']\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explain_pred(idx, fs):\n",
    "    c = make_pipeline(vectorizers[fs], classifiers[fs])\n",
    "    exp = explainer.explain_instance(test_texts[fs][idx].lower(), c.predict_proba, num_features=8)\n",
    "    print('Patch id: %d' % idx)\n",
    "    print('Probability(bug) =', c.predict_proba([test_texts[fs][idx]])[0,1])\n",
    "    print('True class: %s' % class_names[test_targets[fs][idx]])\n",
    "    print('Text: %s' % test_texts[fs][idx])\n",
    "    pp.pprint(exp.as_list())\n",
    "    # exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "fs = 'ext3'\n",
    "c = make_pipeline(vectorizers[fs], classifiers[fs])\n",
    "for i in range(len(test_texts[fs])):\n",
    "    if ('fix' not in test_texts[fs][i].lower() \n",
    "        and c.predict_proba([test_texts[fs][i]])[0,1] > 0.5\n",
    "        and test_targets[fs][i] == 1):\n",
    "        explain_pred(i, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keyword 'fix'\n",
    "for i in [22, 24]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when keyword 'fix' is absent\n",
    "for i in [23, 25]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# interesting case\n",
    "for i in [5, 26]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
