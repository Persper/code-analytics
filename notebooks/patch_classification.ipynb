{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pickle\n",
    "datasets = pickle.load(open('data/fs-patch.pickle', 'rb'))\n",
    "fss = ['ext3', 'ext4', 'btrfs', 'xfs', 'jfs', 'reiserfs']\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Training classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone, is_classifier\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize, binarize, normalize, LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline, make_union\n",
    "import functools\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "rf = functools.partial(sklearn.ensemble.RandomForestClassifier, n_estimators=300)\n",
    "\n",
    "def binary_bug(dp):\n",
    "    if dp['type'] == 'b':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def multi_patch_type(dp):\n",
    "    return dp['type']\n",
    "\n",
    "def multi_bug_cons(dp):\n",
    "    return dp['cons_type']\n",
    "\n",
    "def only_bug_filter(dp):\n",
    "    return dp['type'] == 'b'\n",
    "        \n",
    "def lemmatize(text):\n",
    "    lemmatized = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PUNCT':\n",
    "            lemmatized.append(token.lemma_)\n",
    "    return lemmatized\n",
    "\n",
    "def stem(text):\n",
    "    stemmed = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed\n",
    "\n",
    "def bns(tprs, fprs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tprs: A row vector of shape (1, num_features)\n",
    "        fprs: A row vector of shape (1, num_features)\n",
    "    \"\"\"\n",
    "    num_features = tprs.shape[1]\n",
    "    bns = np.zeros(num_features)\n",
    "    for i in range(num_features):\n",
    "        bns[i] = np.abs(norm.ppf(tprs[0, i]) - norm.ppf(fprs[0, i]))\n",
    "    return bns\n",
    "\n",
    "def tfbns(tfs, bns):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tfs: A sparse matrix of shape (num_samples, num_features)\n",
    "            in csr format\n",
    "        bns: A numpy array of shape (num_features,)\n",
    "    \"\"\"\n",
    "    cx = tfs.tocoo()\n",
    "    data, rows, cols = [], [], []\n",
    "    for i, j, v in zip(cx.row, cx.col, cx.data):\n",
    "        data.append(v * bns[j])\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "    return csr_matrix((data, (rows, cols)), shape=tfs.shape)\n",
    "\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, use_bns):\n",
    "        self.use_bns = use_bns\n",
    "        if self.use_bns:\n",
    "            self.tf_trans = TfidfTransformer(use_idf=False, norm=None)\n",
    "        else:\n",
    "            self.tf_trans = TfidfTransformer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.tf_trans.fit(X)\n",
    "        \n",
    "        if self.use_bns:\n",
    "            binary_counts = binarize(X)\n",
    "\n",
    "            pos = np.sum(y)\n",
    "            neg = np.size(y) - pos\n",
    "\n",
    "            tps = np.sum(binary_counts[np.nonzero(y)[0]], axis=0)\n",
    "            fps = np.sum(binary_counts[np.argwhere(y == 0)[:, 0]], axis=0)\n",
    "\n",
    "            tprs = np.clip(tps / pos, 0.0005, 0.9995)\n",
    "            fprs = np.clip(fps / neg, 0.0005, 0.9995)\n",
    "\n",
    "            self.bns_values = bns(tprs, fprs)\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def transform(self, counts):\n",
    "        tfs = self.tf_trans.transform(counts)\n",
    "        if self.use_bns:\n",
    "            return normalize(tfbns(tfs, self.bns_values))\n",
    "        else:\n",
    "            return tfs\n",
    "        \n",
    "class FeatureLabelExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Cannot be used in Pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets, text_feature, label_func, dp_filter):\n",
    "        self.datasets = datasets\n",
    "        self.text_feature = text_feature\n",
    "        self.label_func = label_func\n",
    "        self.dp_filter = dp_filter\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, fs_list):\n",
    "        num_samples = sum([\n",
    "            sum([1 for dp in self.datasets[fs] if self.dp_filter(dp)]) \n",
    "            for fs in fs_list])\n",
    "        \n",
    "        features = {}\n",
    "        features['text'] = [None] * num_samples\n",
    "        features['frc'] = np.zeros((num_samples, 3))\n",
    "        labels = [None] * num_samples\n",
    "        ind = 0\n",
    "        for fs in fs_list:\n",
    "            for dp in self.datasets[fs]:\n",
    "                if self.dp_filter(dp):\n",
    "                    features['text'][ind] = dp[self.text_feature]\n",
    "                    features['frc'][ind] = np.array([dp['num_files'],\n",
    "                                                     dp['num_adds'],\n",
    "                                                     dp['num_dels']])\n",
    "                    labels[ind] = self.label_func(dp)\n",
    "                    ind += 1\n",
    "        return features, labels\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "def _fit_binary(estimator, use_bns,\n",
    "                use_text, use_frc, k, X, y):\n",
    "    estimator = clone(estimator)\n",
    "    text = make_pipeline(\n",
    "        ItemSelector(key='count'), TextTransformer(use_bns=use_bns))\n",
    "    frc = make_pipeline(\n",
    "        ItemSelector(key='frc'), StandardScaler())\n",
    "        \n",
    "    if use_text and use_frc:\n",
    "        union = make_union(text, frc)\n",
    "    elif not use_text and use_frc:\n",
    "        union = frc\n",
    "    elif use_text and not use_frc:\n",
    "        union = text\n",
    "        \n",
    "    if k:\n",
    "        pipeline = make_pipeline(union,\n",
    "                                 SelectKBest(mutual_info_classif, k=k),\n",
    "                                 estimator)\n",
    "    else:\n",
    "        pipeline = make_pipeline(union, estimator)\n",
    "    \n",
    "    pipeline.fit(X, y)\n",
    "    return pipeline \n",
    "\n",
    "def _predict_binary(pipeline, X):\n",
    "    \"\"\"Make predictions using a single binary estimator\"\"\"\n",
    "    try:\n",
    "        score = np.ravel(pipeline.decision_function(X))\n",
    "    except (AttributeError, NotImplementedError):\n",
    "        score = pipeline.predict_proba(X)[:, 1]\n",
    "    return score\n",
    "    \n",
    "\n",
    "class BNSClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, estimator, use_bns, \n",
    "                 use_text, use_frc, k, n_jobs=1):\n",
    "        assert(use_text or use_frc)\n",
    "        self.estimator = estimator\n",
    "        self.n_jobs = n_jobs\n",
    "        self.use_bns = use_bns\n",
    "        self.use_text = use_text\n",
    "        self.use_frc = use_frc\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.label_binarizer = LabelBinarizer(sparse_output=True)\n",
    "        Y = self.label_binarizer.fit_transform(y)\n",
    "        Y = Y.tocsc()\n",
    "        self.classes = self.label_binarizer.classes_\n",
    "        columns = (col.toarray().ravel() for col in Y.T)\n",
    "        \n",
    "        self.pipelines = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_binary)(\n",
    "                self.estimator, self.use_bns, \n",
    "                self.use_text, self.use_frc, self.k, X, column)\n",
    "            for column in columns)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if(hasattr(self.pipelines[0], 'decision_function') and\n",
    "              is_classifier(self.pipelines[0])):\n",
    "            thresh = 0\n",
    "        else:\n",
    "            thresh = 0.5\n",
    "            \n",
    "        n_samples = X['count'].shape[0]\n",
    "        if self.label_binarizer.y_type_ == 'multiclass':\n",
    "            maxima = np.empty(n_samples, dtype=float)\n",
    "            maxima.fill(-np.inf)\n",
    "            argmaxima = np.zeros(n_samples, dtype=int)\n",
    "            for i, p in enumerate(self.pipelines):\n",
    "                pred = _predict_binary(p, X)\n",
    "                np.maximum(maxima, pred, out=maxima)\n",
    "                argmaxima[maxima == pred] = i\n",
    "            return self.classes[np.array(argmaxima.T)]\n",
    "        else:\n",
    "            indices = array.array('i')\n",
    "            indptr = array.array('i', [0])\n",
    "            for p in self.pipelines:\n",
    "                indices.extend(np.where(_predict_binary(p, X) > thresh)[0])\n",
    "                indptr.append(len(indices))\n",
    "            data = np.ones(len(indices), dtype=int)\n",
    "            indicator = csc_matrix((data, indices, indptr),\n",
    "                shape=(n_samples, len(self.pipelines)))\n",
    "            return self.label_binarizer.inverse_transform(indicator)\n",
    "\n",
    "\n",
    "class Experiment():\n",
    "    \n",
    "    def __init__(self, datasets, fss):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datasets: A dictionary, keys are dataset names, each dataset\n",
    "                is a list of data points (also dictionaries).\n",
    "            fss: A list of file system names.\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.fss = fss\n",
    "\n",
    "    def _clean(self):\n",
    "        self.feature_labels = {}\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def run(self, label_func, estimator, ngram_range=(1, 1),\n",
    "            text_feature='message', use_text=True, use_frc=False,\n",
    "            dp_filter=lambda dp: True, tokenizer=None, \n",
    "            max_features=None, min_df=1, use_bns=False, k=None,\n",
    "            n_jobs=1):\n",
    "        \"\"\"Perform experiment in a leave-one-out style\n",
    "        \n",
    "        Args:\n",
    "            label_func: A function, takes a data point as input and\n",
    "                return its target label.\n",
    "            estimator: A function, return a classifier which supports\n",
    "                'fit' and 'predict' method\n",
    "            ngram_range: A tuple of two integers, specify what range of \n",
    "                ngram to use\n",
    "            text_feature: A string, can be either 'message' or 'subject'.\n",
    "                If set to None, then texts will not be used.\n",
    "            use_text: A boolean flag, whether to use text feature\n",
    "            use_frc: A boolean flag, whether to use frc\n",
    "            dp_filter: A function decides which data point to exclude.\n",
    "            tokenizer: A function takes a string and return a list of tokens.\n",
    "            max_features: An int or None. If not None, only consider\n",
    "                top max_features ordered by term frequency across the corpus.\n",
    "            min_df: An int, ignore terms when building vocabulary if their\n",
    "                document frequency is strictly lower than this threshold.\n",
    "            use_bns: A boolean flag, use BNS if True, otherwise use IDF\n",
    "            k: An int, number of top features to keep during feature\n",
    "                selection. \n",
    "        \"\"\"\n",
    "        self._clean()\n",
    "        \n",
    "        fle = FeatureLabelExtractor(\n",
    "            self.datasets, text_feature, label_func, dp_filter)\n",
    "\n",
    "        for fs in self.fss:\n",
    "            ofs_list = [ofs for ofs in self.fss if ofs != fs]\n",
    "\n",
    "            train_X, train_y = fle.transform(ofs_list)\n",
    "            test_X, test_y = fle.transform([fs])\n",
    "\n",
    "            cv = CountVectorizer(tokenizer=tokenizer,\n",
    "                                 ngram_range=ngram_range,\n",
    "                                 max_features=max_features,\n",
    "                                 min_df=min_df)\n",
    "            \n",
    "            train_X['count'] = cv.fit_transform(train_X['text'])\n",
    "            test_X['count'] = cv.transform(test_X['text'])\n",
    "\n",
    "            # TODO set n_jobs\n",
    "            clf = BNSClassifier(estimator(), \n",
    "                                use_bns=use_bns,\n",
    "                                use_text=use_text,\n",
    "                                use_frc=use_frc,\n",
    "                                k=k,\n",
    "                                n_jobs=n_jobs)\n",
    "            clf.fit(train_X, train_y)\n",
    "\n",
    "            print('----- Test Accuracy for %s -----' % fs)\n",
    "            print('Classifier: %.3f' % clf.score(test_X, test_y))\n",
    "            \n",
    "            self.classifiers[fs] = clf\n",
    "            \n",
    "exp = Experiment(datasets, fss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Binary Classification of Bug Fix Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using frc\n",
    "exp.run(binary_bug, LinearSVC, use_text=False, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * IDF)\n",
    "exp.run(binary_bug, LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * BNS)\n",
    "exp.run(binary_bug, LinearSVC, use_bns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * IDF) and frc\n",
    "exp.run(binary_bug, LinearSVC, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "exp.run(binary_bug, LinearSVC, use_bns=True, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * IDF) and frc\n",
    "# drop terms with frequency lower than min_df\n",
    "exp.run(binary_bug, LinearSVC, min_df=3, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "# drop terms with frequency lower than min_df\n",
    "exp.run(binary_bug, LinearSVC, use_bns=True, min_df=3, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both stemmed text (TF * BNS) and frc\n",
    "exp.run(binary_bug, LinearSVC, use_bns=True, use_frc=True, tokenizer=stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both lemmatized text (TF * BNS) and frc\n",
    "exp.run(binary_bug, LinearSVC, use_bns=True, use_frc=True, tokenizer=lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "# select top k features by mutual information\n",
    "exp.run(binary_bug, LinearSVC, use_bns=True, use_frc=True, k=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multi-Class Classification of Patch Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using frc\n",
    "exp.run(multi_patch_type, LinearSVC, use_text=False, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * IDF)\n",
    "exp.run(multi_patch_type, LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * BNS)\n",
    "exp.run(multi_patch_type, LinearSVC, use_bns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * IDF) and frc\n",
    "exp.run(multi_patch_type, LinearSVC, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "exp.run(multi_patch_type, LinearSVC, use_bns=True, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "# drop terms with frequency lower than min_df\n",
    "exp.run(multi_patch_type, LinearSVC, use_bns=True, min_df=3, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "# select top k features by mutual information\n",
    "exp.run(multi_patch_type, LinearSVC, use_bns=True, use_frc=True, k=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multi-Class Classification of Bug Consequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * IDF)\n",
    "exp.run(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * BNS)\n",
    "exp.run(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter, use_bns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * IDF) and frc\n",
    "exp.run(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both text (TF * BNS) and frc\n",
    "exp.run(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter, \n",
    "        use_bns=True, use_frc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# using both bigram text (TF * BNS) and frc\n",
    "exp.run(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter, \n",
    "        use_bns=True, use_frc=True, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only using text (TF * IDF)\n",
    "# select top k features by mutual information\n",
    "exp.run(multi_bug_cons, LinearSVC, dp_filter=only_bug_filter, k=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explaning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# explaning random forest classifier\n",
    "def rank_features_by_importance(exp, top_n=20):\n",
    "    if 'feature_importances_' in dir(exp.classifiers['ext3']):\n",
    "        for fs in fss:\n",
    "            print('------- important features for %s -------' % fs)\n",
    "            truncated_importances = map(lambda x: '%.4f' % x, exp.classifiers[fs].feature_importances_)\n",
    "            pp.pprint(sorted(zip(truncated_importances, exp.feature_labels[fs]), reverse=True)[:top_n])\n",
    "    else:\n",
    "        print(\"classifiers don't have attribute feature_importance_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rank_features_by_importance(exp, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explaining SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# explaining linear SVM classifier\n",
    "def rank_features_by_weight(classifiers, classes, feature_labels, \n",
    "                            top_n=20, individual_class=False):\n",
    "    \"\"\"Rank features by the absolute value of associated primal weight\n",
    "    \n",
    "    Args:\n",
    "        classifiers: A dictionary, with key being the file system name\n",
    "            and value being the corresponding classifier.\n",
    "        classes: A list of class labels.\n",
    "        feature_labels: A dictionary of list of feature names, each key\n",
    "            is a file system name, features are in the order as classifier\n",
    "            sees them.\n",
    "        top_n: An integer, specifies number of top features to print.\n",
    "        individual_class: A boolean value, whether use sum of absolute weights \n",
    "            across classes. In binary tasks, this value doesn't matter.\n",
    "    \"\"\"\n",
    "    \n",
    "    fss = list(classifiers.keys())\n",
    "    if 'coef_' in dir(classifiers[fss[0]]):\n",
    "        for fs in fss:\n",
    "            print('------- %s -------' % fs)\n",
    "            if individual_class:\n",
    "                coef = classifiers[fs].coef_\n",
    "                for i in range(coef.shape[0]):\n",
    "                    print('\\t------ %s ------' % classes[i])\n",
    "                    # print('\\t------  ------')\n",
    "                    order = np.argsort(np.absolute(coef[i]))\n",
    "                    pp.pprint(np.array(feature_labels[fs])[order][-top_n:])\n",
    "                print()\n",
    "            else:\n",
    "                coef = classifiers[fs].coef_\n",
    "                num_features = coef.shape[1]\n",
    "                abs_coef_sum = np.zeros(num_features)\n",
    "                for i in range(coef.shape[0]):\n",
    "                    abs_coef_sum += np.absolute(coef[i])\n",
    "                order = np.argsort(abs_coef_sum)\n",
    "                pp.pprint(np.array(feature_labels[fs])[order][-top_n:])\n",
    "    else:\n",
    "        print(\"classifiers don't have attribute coef_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rank_features_by_weight(exp.classifiers, np.unique(exp.train_targets['ext3']), \n",
    "                        exp.feature_labels, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rank_features_by_weight(exp.classifiers, np.unique(exp.train_targets['ext3']), \n",
    "                        exp.feature_labels, top_n=20, individual_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizers['ext3'], classifiers['ext3'])\n",
    "print(c.predict_proba([test_texts['ext3'][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names = ['not-bug', 'bug']\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def explain_pred(idx, fs):\n",
    "    c = make_pipeline(vectorizers[fs], classifiers[fs])\n",
    "    exp = explainer.explain_instance(test_texts[fs][idx].lower(), c.predict_proba, num_features=8)\n",
    "    print('Patch id: %d' % idx)\n",
    "    print('Probability(bug) =', c.predict_proba([test_texts[fs][idx]])[0,1])\n",
    "    print('True class: %s' % class_names[test_targets[fs][idx]])\n",
    "    print('Text: %s' % test_texts[fs][idx])\n",
    "    pp.pprint(exp.as_list())\n",
    "    # exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "fs = 'ext3'\n",
    "c = make_pipeline(vectorizers[fs], classifiers[fs])\n",
    "for i in range(len(test_texts[fs])):\n",
    "    if ('fix' not in test_texts[fs][i].lower() \n",
    "        and c.predict_proba([test_texts[fs][i]])[0,1] > 0.5\n",
    "        and test_targets[fs][i] == 1):\n",
    "        explain_pred(i, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# keyword 'fix'\n",
    "for i in [22, 24]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# when keyword 'fix' is absent\n",
    "for i in [23, 25]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# interesting case\n",
    "for i in [5, 26]:\n",
    "    explain_pred(i, 'ext3')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
